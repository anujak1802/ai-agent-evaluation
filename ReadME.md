### ğŸ§  AI Agent Evaluation Platform 
A full end-to-end evaluation framework for benchmarking AI agents across multiple models, prompts, and
tasks---complete with: â€¢ FastAPI backend â€¢ PostgreSQL database â€¢
Background evaluation worker â€¢ Real-time dashboard UI â€¢ Chart.js
visualizations (latency, score) â€¢ Multi-agent + multi-testcase
evaluation runs â€¢ Auto-refreshing frontend This system lets you run
structured evaluations across agents, compare performance, visualize
results, and track metrics like latency, cost, and accuracy scoring.

## ğŸš€ Features

-  âœ… 1. Agents Agents represent AI models you want to
benchmark. Each agent has: â€¢ name â€¢ model (e.g., GPT-4o, GPT-4o-mini) â€¢
config (temperature, max_tokens, etc.) â€¢ created_at API POST /agents/ â†’
create agentGET /agents/ â†’ list all agents

- âœ… 2. Testcases Testcases define the evaluation prompts. Each testcase
includes: â€¢ name â€¢ prompt â€¢ (optional) expected answer â€¢ (optional) meta
for evaluating correctness API POST /testcases/ â†’ create testcaseGET
/testcases/ â†’ list testcases

- âœ… 3. Evaluation Runs An evaluation run pairs multiple agents with
multiple testcases. Example: Evaluate Agent 1 and Agent 2 on testcases
1, 2, and 3. The platform automatically: â€¢ creates a new run â€¢ marks
status: pending â†’ running â†’ complete â€¢ triggers worker jobs â€¢ produces
per-agent-per-testcase results API POST /evals/
```
{ \"agent_ids\": \[1, 2\], \"testcase_ids\": \[1, 2, 3\], \"run_name\":
\"math-eval-benchmark-1\" } GET /evals/ â†’ List runs
```
- âœ… 4. Evaluation Worker A background worker performs the actual
evaluation: For every (agent Ã— testcase) pair: 1 Calls the LLM (OpenAI
or local) 2 Records response text 3 Computes: â—¦ score â—¦ latency_ms â—¦
cost_usd 4 Writes results into database 5 Marks run as complete

- âœ… 5. Results Every model/testcase output is stored as a row: â€¢ run_id â€¢
agent_id â€¢ testcase_id â€¢ response â€¢ score â€¢ latency_ms â€¢ cost_usd API
POST /results/ â†’ worker uses to store outputGET /results/{run_id} â†’
dashboard uses to fetch chart data

### ğŸ“Š Dashboard UI Accessible at: ğŸ‘‰ /dashboard The dashboard displays:
Agents Table â€¢ ID â€¢ Name â€¢ Model â€¢ Created Evaluation Runs â€¢ Run name â€¢
Status (badges: pending / running / completed) â€¢ Agent ID â€¢ Click to
view results Run Details Shows detailed per-testcase results: â€¢ Response
â€¢ Score â€¢ Latency (ms) â€¢ Cost Charts Powered by Chart.js: Score Bar
Chart Visualizes model accuracy across testcases. Latency Line Chart
Shows model speed across prompts. Automatically updates as results come
in.


ğŸ§ª Example Workflow 
 - 1ï¸âƒ£ Create Agents

```
 curl -X POST http://localhost:8000/agents/ \\ -H \"Content-Type:
application/json\" \\ -d \'{ \"name\": \"math-agent-4o-mini\",
\"model\": \"gpt-4o-mini\", \"config\": {\"temperature\": 0.0} }\' 
```
- 2ï¸âƒ£ Create Testcases
```
curl -X POST http://localhost:8000/testcases/ \\ -H \"Content-Type:
application/json\" \\ -d \'{ \"name\": \"addition\", \"prompt\": \"What
is 2+2?\" }\' 
```
- 3ï¸âƒ£ Start Evaluation Run
```
curl -X POST http://localhost:8000/evals/ \\ -H \"Content-Type:
application/json\" \\ -d \'{ \"agent_ids\": \[1, 2\], \"testcase_ids\":
\[1, 2, 3\], \"run_name\": \"math-eval-benchmark-1\" }\'
```
-  4ï¸âƒ£ Worker Computes Results Automatically or manually:
```
curl -X POST http://localhost:8000/results/ \\ -H \"Content-Type:
application/json\" \\ -d \'{ \"run_id\": 1, \"agent_id\": 1,
\"testcase_id\": 1, \"response\": \"4\", \"score\": 1.0, \"latency_ms\":
220, \"cost_usd\": 0.00015 }\'
```
### ğŸ–¥ï¸ Run Locally (Docker)
```
docker compose up 
```
-- build Services: â€¢ api-1 â†’ FastAPI backend â€¢ db-1 â†’
PostgreSQL â€¢ worker-1 â†’ evaluation worker â€¢ dashboard â†’ served from
/static/dashboard.html

ğŸ“ Project Structure
```
app/ 
â”œâ”€â”€ api/ 
â”‚ â”œâ”€â”€ routes_agents.py 
â”‚ â”œâ”€â”€ routes_evals.py 
â”‚ â”œâ”€â”€routes_results.py 
â”‚ â””â”€â”€ routes_testcases.py 
â”œâ”€â”€ db/ 
â”‚ â”œâ”€â”€ models.py 
â”‚ â””â”€â”€ session.py 
â”œâ”€â”€ workers/ 
â”‚ â””â”€â”€ eval_worker.py 
â”œâ”€â”€ static/ 
â”‚ â””â”€â”€
dashboard.html 
â””â”€â”€ main.py
```

### ğŸ’¡ Roadmap Ideas 
â€¢ âœ“ Add multi-run comparison charts â€¢ âœ“ Add cost trend
chart â€¢ âœ“ Add filters (model, date, run name) â€¢ ğŸ”² Add chart exporting
(PNG/PDF) â€¢ ğŸ”² Add authentication â€¢ ğŸ”² Add distributed worker queue
(Celery, Redis) â€¢ ğŸ”² Add real expected-answer scoring (string
similarity, embeddings, etc.)

ğŸ™Œ Contributing Issues and PRs are welcome!If you need help extending
the system, just ask.

If you want, I can also generate: âœ… A pretty Markdown version with
logos & emojisâœ… A developer setup READMEâœ… A docs site (mkdocs)Just
tell me!

You said: README.md that I can directly paste

ChatGPT said: Here is a clean, ready-to-paste README.md---no
placeholders, no explanations needed.Copy/paste it directly into your
repo.
